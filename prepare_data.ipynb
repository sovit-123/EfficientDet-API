{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introductory-stereo",
   "metadata": {},
   "source": [
    "## <u>Introduction and Note</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "charming-encyclopedia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘trainer/aug_sample_images’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir \"trainer/aug_sample_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "returning-country",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir validation_image_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-logic",
   "metadata": {},
   "source": [
    "## <u>Prepare Data</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "designed-vertical",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob as glob\n",
    "import random\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "under-awareness",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copy the images and XML files from original folders to the temporary folders\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "ROOT_PATH = \"./\" # YOU MAY NEED TO CHNAGE THIS\n",
    "DATA_FOLDER = 'data'\n",
    "\n",
    "#################################\n",
    "# chage the path after {ROOT_PATH}/\n",
    "os.makedirs(f\"{ROOT_PATH}/{DATA_FOLDER}\", exist_ok=True) \n",
    "os.makedirs(f\"{ROOT_PATH}/{DATA_FOLDER}/train\", exist_ok=True)\n",
    "os.makedirs(f\"{ROOT_PATH}/{DATA_FOLDER}/validation\", exist_ok=True)\n",
    "os.makedirs(f\"{ROOT_PATH}/{DATA_FOLDER}/test\", exist_ok=True)\n",
    "os.makedirs(f\"{ROOT_PATH}/{DATA_FOLDER}/train/images\", exist_ok=True)\n",
    "os.makedirs(f\"{ROOT_PATH}/{DATA_FOLDER}/train/annotations\", exist_ok=True)\n",
    "os.makedirs(f\"{ROOT_PATH}/{DATA_FOLDER}/validation/images\", exist_ok=True)\n",
    "os.makedirs(f\"{ROOT_PATH}/{DATA_FOLDER}/validation/annotations\", exist_ok=True)\n",
    "os.makedirs(f\"{ROOT_PATH}/{DATA_FOLDER}/test/images\", exist_ok=True)\n",
    "os.makedirs(f\"{ROOT_PATH}/{DATA_FOLDER}/test/annotations\", exist_ok=True)\n",
    "os.makedirs(f\"{ROOT_PATH}/{DATA_FOLDER}/train/labels\", exist_ok=True)\n",
    "os.makedirs(f\"{ROOT_PATH}/{DATA_FOLDER}/validation/labels\", exist_ok=True)\n",
    "os.makedirs(f\"{ROOT_PATH}/{DATA_FOLDER}/test/labels\", exist_ok=True)\n",
    "\n",
    "#################################\n",
    "# make final training and validation folders\n",
    "# MOST PROBABLY NO NEED TO CHNAGE THESE\n",
    "os.makedirs(f\"{ROOT_PATH}/final_train_images\", exist_ok=True)\n",
    "os.makedirs(f\"{ROOT_PATH}/final_valid_images\", exist_ok=True)\n",
    "os.makedirs(f\"{ROOT_PATH}/final_test_images\", exist_ok=True)\n",
    "os.makedirs(f\"{ROOT_PATH}/final_labels\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "provincial-revelation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../input/ships_dataset/ShipsPascalVOC/annotations/boat0.xml']\n",
      "['../input/ships_dataset/ShipsPascalVOC/images/boat0.png']\n",
      "Total number of training images: 621\n",
      "Total number of training annotations: 621\n",
      "Total extra images: 0\n",
      "Total extra XMLs: 0\n",
      "Train data points: 496\n",
      "Validation/evaluation data points: 62\n",
      "Test data points: 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/496 [00:00<00:28, 17.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      " \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 496/496 [00:03<00:00, 151.40it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 197.15it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 416.89it/s]\n"
     ]
    }
   ],
   "source": [
    "COPY_FROM_ROOT_PATH = '../input/ships_dataset/ShipsPascalVOC'\n",
    "ANNOT_PATH = f\"{COPY_FROM_ROOT_PATH}/annotations\"\n",
    "IMAGE_PATH = f\"{COPY_FROM_ROOT_PATH}/images\"\n",
    "\n",
    "COPY_TO_ROOT_PATH = 'data'\n",
    "\n",
    "annotations = glob.glob(f\"{ANNOT_PATH}/*.xml\")\n",
    "images = glob.glob(f\"{IMAGE_PATH}/*.png\")\n",
    "\n",
    "annotations.sort()\n",
    "images.sort()\n",
    "\n",
    "image_names = []\n",
    "annot_names = []\n",
    "\n",
    "print(annotations[:1])\n",
    "print(images[:1])\n",
    "\n",
    "print(f\"Total number of training images: {len(images)}\")\n",
    "print(f\"Total number of training annotations: {len(annotations)}\")\n",
    "\n",
    "for image in images:\n",
    "    image_name = image.split('.')[0]\n",
    "    image_names.append(image_name)\n",
    "for annot in annotations:\n",
    "    annot_name = annot.split('.')[0]\n",
    "    annot_names.append(annot_name)\n",
    "    \n",
    "\n",
    "# check for images without XML files\n",
    "extra_image_counter = 0\n",
    "for num, name in enumerate(image_names):\n",
    "    if name not in annot_names:\n",
    "        print(f\"EXTRA IMAGE: {image_name}.png\")\n",
    "        extra_image_counter += 1\n",
    "# check for XML files without images\n",
    "extra_xml_counter = 0\n",
    "for num, name in enumerate(annot_names):\n",
    "    if name not in image_names:\n",
    "        print(f\"EXTRA XML: {annot_name}.xml\")\n",
    "        extra_xml_counter += 1\n",
    "print(f\"Total extra images: {extra_image_counter}\")\n",
    "print(f\"Total extra XMLs: {extra_xml_counter}\")\n",
    "\n",
    "assert (len(annotations) == len(images)), 'Number of images and annotataion files do not match'\n",
    "\n",
    "train_split = 80\n",
    "valid_split = 10\n",
    "test_split = 10\n",
    "\n",
    "# set the train, evaluation/validation, and test ratio\n",
    "train_ratio, valid_ratio, test_ratio = int((train_split/100)*len(images)), int((valid_split/100)*len(images)), int((test_split/100)*len(images))\n",
    "print(f\"Train data points: {train_ratio}\")\n",
    "print(f\"Validation/evaluation data points: {valid_ratio}\")\n",
    "print(f\"Test data points: {test_ratio}\")\n",
    "\n",
    "#############################################################\n",
    "random.seed(42)\n",
    "r = random.random() # randomly generating a real in [0,1)\n",
    "\n",
    "# for training data\n",
    "temp_annotations = glob.glob(f\"{ANNOT_PATH}/*.xml\")\n",
    "temp_images = glob.glob(f\"{IMAGE_PATH}/*.png\")\n",
    "temp_annotations.sort()\n",
    "temp_images.sort()\n",
    "random.shuffle(temp_annotations, lambda : r)  # lambda : r is an unary function which returns r\n",
    "random.shuffle(temp_images, lambda : r) # using the same function as used in prev line so that shuffling order is same\n",
    "for i in tqdm(range(0, train_ratio)):\n",
    "    if i < 5:\n",
    "        print(temp_annotations[i].split('.')[0].split(os.path.sep)[-1], temp_images[i].split('.')[0].split(os.path.sep)[-1])\n",
    "    assert(temp_annotations[i].split('.')[0].split(os.path.sep)[-1] == temp_images[i].split('.')[0].split(os.path.sep)[-1]), 'Annotation and image do not match'\n",
    "    shutil.copy(temp_annotations[i], f\"{COPY_TO_ROOT_PATH}/train/annotations\")\n",
    "    shutil.copy(temp_images[i], f\"{COPY_TO_ROOT_PATH}/train/images\")\n",
    "\n",
    "# for validation dataset\n",
    "temp_annotations = glob.glob(f\"{ANNOT_PATH}/*.xml\")\n",
    "temp_images = glob.glob(f\"{IMAGE_PATH}/*.png\")\n",
    "temp_annotations.sort()\n",
    "temp_images.sort()\n",
    "random.shuffle(temp_annotations, lambda : r)  # lambda : r is an unary function which returns r\n",
    "random.shuffle(temp_images, lambda : r) # using the same function as used in prev line so that shuffling order is same\n",
    "for i in tqdm(range(train_ratio, train_ratio+valid_ratio)):\n",
    "    if i < 5:\n",
    "        print(temp_annotations[i].split('.')[0].split(os.path.sep)[-1], temp_images[i].split('.')[0].split(os.path.sep)[-1])\n",
    "    assert(temp_annotations[i].split('.')[0].split(os.path.sep)[-1] == temp_images[i].split('.')[0].split(os.path.sep)[-1]), 'Annotation and image do not match'\n",
    "    shutil.copy(temp_annotations[i], f\"{COPY_TO_ROOT_PATH}/validation/annotations\")\n",
    "    shutil.copy(temp_images[i], f\"{COPY_TO_ROOT_PATH}/validation/images\")\n",
    "\n",
    "# for test data\n",
    "temp_annotations = glob.glob(f\"{ANNOT_PATH}/*.xml\")\n",
    "temp_images = glob.glob(f\"{IMAGE_PATH}/*.png\")\n",
    "temp_annotations.sort()\n",
    "temp_images.sort()\n",
    "random.shuffle(temp_annotations, lambda : r)  # lambda : r is an unary function which returns r\n",
    "random.shuffle(temp_images, lambda : r) # using the same function as used in prev line so that shuffling order is same\n",
    "for i in tqdm(range(train_ratio+valid_ratio, train_ratio+valid_ratio+test_ratio)):\n",
    "    if i < 5:\n",
    "        print(temp_annotations[i].split('.')[0].split(os.path.sep)[-1], temp_images[i].split('.')[0].split(os.path.sep)[-1])\n",
    "    assert(temp_annotations[i].split('.')[0].split(os.path.sep)[-1] == temp_images[i].split('.')[0].split(os.path.sep)[-1]), 'Annotation and image do not match'\n",
    "    shutil.copy(temp_annotations[i], f\"{COPY_TO_ROOT_PATH}/test/annotations\")\n",
    "    shutil.copy(temp_images[i], f\"{COPY_TO_ROOT_PATH}/test/images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "spatial-civilian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted xml to csv.\n",
      "Successfully converted xml to csv.\n",
      "Successfully converted xml to csv.\n"
     ]
    }
   ],
   "source": [
    "def xml_to_csv(path):\n",
    "    annotations_list = glob.glob(path + '/*.xml')\n",
    "    annotations_list.sort()\n",
    "    xml_list = []\n",
    "    for xml_file in annotations_list:\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        for member in root.findall('object'):\n",
    "            # print('MEMBER', member)\n",
    "            try:\n",
    "                value = (root.find('filename').text,\n",
    "                         int(root.find('size')[0].text),\n",
    "                         int(root.find('size')[1].text),\n",
    "                         member[0].text,\n",
    "                         int(member[4][0].text),\n",
    "                         int(member[4][1].text),\n",
    "                         int(member[4][2].text),\n",
    "                         int(member[4][3].text)\n",
    "                         )\n",
    "            except:\n",
    "                value = (root.find('filename').text,\n",
    "                         int(root.find('size')[0].text),\n",
    "                         int(root.find('size')[1].text),\n",
    "                         member[0].text,\n",
    "                         int(member[5][0].text),\n",
    "                         int(member[5][1].text),\n",
    "                         int(member[5][2].text),\n",
    "                         int(member[5][3].text)\n",
    "                         )\n",
    "            xml_list.append(value)\n",
    "    column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
    "    return xml_df\n",
    "\n",
    "\n",
    "def main():\n",
    "    ROOT_PATH = 'data'\n",
    "    for folder in [\n",
    "        f\"{ROOT_PATH}/train\", \n",
    "        f\"{ROOT_PATH}/validation\",\n",
    "        f\"{ROOT_PATH}/test\"]:\n",
    "        xml_df = xml_to_csv(folder+'/'+'annotations')\n",
    "        xml_df.to_csv((folder + '/' + 'labels/' + 'labels.csv'), index=None)\n",
    "        print('Successfully converted xml to csv.')\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "subject-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = 'data'\n",
    "\n",
    "train_csv = pd.read_csv(f\"{ROOT_PATH}/train/labels/labels.csv\")\n",
    "final_train_csv = pd.concat([train_csv])\n",
    "final_train_csv.to_csv(f\"final_labels/final_train_csv.csv\", index=False)\n",
    "\n",
    "valid_csv = pd.read_csv(f\"{ROOT_PATH}/validation/labels/labels.csv\")\n",
    "final_valid_csv = pd.concat([valid_csv])\n",
    "final_valid_csv.to_csv(f\"final_labels/final_valid_csv.csv\", index=False)\n",
    "\n",
    "test_csv = pd.read_csv(f\"{ROOT_PATH}/test/labels/labels.csv\")\n",
    "final_test_csv = pd.concat([test_csv])\n",
    "final_test_csv.to_csv(f\"final_labels/final_test_csv.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "natural-tribune",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 496/496 [00:01<00:00, 289.53it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 313.09it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 778.50it/s]\n"
     ]
    }
   ],
   "source": [
    "ROOT_PATH = 'data'\n",
    "\n",
    "# for training images\n",
    "train_images = glob.glob(f\"{ROOT_PATH}/train/images/*.png\")\n",
    "\n",
    "for i in tqdm(range(len(train_images))):\n",
    "    shutil.copy(train_images[i], f\"final_train_images\")\n",
    "\n",
    "#######################\n",
    "#######################\n",
    "# for validation images\n",
    "\n",
    "valid_images = glob.glob(f\"{ROOT_PATH}/validation/images/*.png\")\n",
    "\n",
    "for i in tqdm(range(len(valid_images))):\n",
    "    shutil.copy(valid_images[i], f\"final_valid_images\")\n",
    "\n",
    "#######################\n",
    "#######################\n",
    "# for test images\n",
    "test_images = glob.glob(f\"{ROOT_PATH}/test/images/*.png\")\n",
    "\n",
    "for i in tqdm(range(len(test_images))):\n",
    "    shutil.copy(test_images[i], f\"final_test_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "tough-cylinder",
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: In test CSV\n",
      "Drop indices: []\n",
      "INFO: In valid CSV\n",
      "Drop indices: []\n",
      "INFO: In training CSV\n",
      "Drop indices: []\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sometimes the xmin, ymin, xmax, and ymax annotation values may be \n",
    "larger than the actual image dimension due to labelling error.\n",
    "This might cause unkown errors down the line. This Python script\n",
    "removes such erroneous rows from the CSV files and rewrites them \n",
    "to the disk.\n",
    "\"\"\"\n",
    "\n",
    "# ROOT_PATH = '/content'\n",
    "\n",
    "print('INFO: In test CSV')\n",
    "test_df = pd.read_csv(f\"final_labels/final_test_csv.csv\")\n",
    "drop_indices = []\n",
    "\n",
    "for i in range(len(test_df)):\n",
    "    w, h = test_df['width'][i], test_df['height'][i]\n",
    "    if test_df['xmin'][i] > w or test_df['xmax'][i] > w or \\\n",
    "        test_df['ymin'][i] > h or test_df['ymax'][i] > h:\n",
    "        print(test_df['filename'][i])\n",
    "        drop_indices.append(test_df.index[i])\n",
    "print(f\"Drop indices: {drop_indices}\")\n",
    "\n",
    "test_df.drop(drop_indices, inplace=True)\n",
    "# REWRITE THE CSV FILE TO DISK. VERY IMPORTANT\n",
    "test_df.to_csv(f\"final_labels/final_test_csv.csv\", index=False)\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "print('INFO: In valid CSV')\n",
    "valid_df = pd.read_csv(f\"final_labels/final_valid_csv.csv\")\n",
    "drop_indices = []\n",
    "\n",
    "for i in range(len(valid_df)):\n",
    "    w, h = valid_df['width'][i], valid_df['height'][i]\n",
    "    if valid_df['xmin'][i] > w or valid_df['xmax'][i] > w \\\n",
    "        or valid_df['ymin'][i] > h or valid_df['ymax'][i] > h:\n",
    "        print(valid_df['filename'][i])\n",
    "        drop_indices.append(valid_df.index[i])\n",
    "print(f\"Drop indices: {drop_indices}\")\n",
    "\n",
    "valid_df.drop(drop_indices, inplace=True)\n",
    "# REWRITE THE CSV FILE TO DISK. VERY IMPORTANT\n",
    "valid_df.to_csv(f\"final_labels/final_valid_csv.csv\", index=False)\n",
    "##########################################################################\n",
    "\n",
    "print('INFO: In training CSV')\n",
    "train_df = pd.read_csv(f\"final_labels/final_train_csv.csv\")\n",
    "drop_indices = []\n",
    "\n",
    "for i in range(len(train_df)):\n",
    "    w, h = train_df['width'][i], train_df['height'][i]\n",
    "    if train_df['xmin'][i] > w or train_df['xmax'][i] > w \\\n",
    "        or train_df['ymin'][i] > h or train_df['ymax'][i] > h:\n",
    "        print(train_df['filename'][i])\n",
    "        drop_indices.append(train_df.index[i])\n",
    "print(f\"Drop indices: {drop_indices}\")\n",
    "\n",
    "train_df.drop(drop_indices, inplace=True)\n",
    "# REWRITE THE CSV FILE TO DISK. VERY IMPORTANT\n",
    "train_df.to_csv(f\"final_labels/final_train_csv.csv\", index=False)\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "smaller-therapy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of final train images: 496\n",
      "Total number of final validation images: 62\n",
      "Total number of final test/evaluation images: 62\n",
      "CLASSES: ['boat'] \n",
      "TOTAL: 1 classes\n",
      "CLASSES: ['boat']\n",
      "CLASSES: ['boat']\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('final_labels/final_train_csv.csv')\n",
    "valid_df = pd.read_csv('final_labels/final_valid_csv.csv')\n",
    "test_df = pd.read_csv('final_labels/final_test_csv.csv')\n",
    "\n",
    "print(f\"Total number of final train images: {len(train_df.groupby(by='filename').count())}\")\n",
    "print(f\"Total number of final validation images: {len(valid_df.groupby(by='filename').count())}\")\n",
    "print(f\"Total number of final test/evaluation images: {len(test_df.groupby(by='filename').count())}\")\n",
    "\n",
    "print(f\"CLASSES: {train_df['class'].unique()} \\nTOTAL: {len(train_df['class'].unique())} classes\")\n",
    "print(f\"CLASSES: {valid_df['class'].unique()}\")\n",
    "print(f\"CLASSES: {test_df['class'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-reliance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
